{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM Demo: univariate stimulus, biophysical dynamics\n",
    "\n",
    "This is a demo notebook for the GLM estimation algorithm. Here we simulate data using the biophysical dynamical cascade model, and then try to estimate parameters with the GLM. The GLM does not have a membrane, so the estimated RF should be the convolution of the input kernel ($k1$) with the membrane kernel ($k2$) which is approximately an exponential decay with time constant $\\tau_m$. However, what can happen is that the kernel gets shifted to shorter lags for the phasic model, which is consistent with what Chen and Meliza (2018) found for the coherence between an input current and the spiking output - the dynamics act as a bandpass filter.\n",
    "\n",
    "This notebook also introduces the use of a raised cosine basis set to reduce the number of parameters in the kernel estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import sys\n",
    "import imp\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.signal as sps\n",
    "import quickspikes as qs\n",
    "\n",
    "import mat_neuron._model as mat\n",
    "from dstrf import strf, mle, filters\n",
    "import spyks.core as spkc\n",
    "\n",
    "# plotting packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # plotting functions\n",
    "import seaborn as sns           # data visualization package\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dt = 0.5\n",
    "stim_dt = 10.0\n",
    "\n",
    "# decay times for exponential adaptation basis set\n",
    "ataus = np.asarray([10, 200], dtype='d')\n",
    "\n",
    "# convolution kernel\n",
    "ntau = 60\n",
    "upsample = int(stim_dt / model_dt)\n",
    "ntbas = 8\n",
    "kscale = 2.0\n",
    "\n",
    "k1, kt = filters.gammadiff(ntau * stim_dt / 32, ntau * stim_dt / 16, 5, ntau * stim_dt, stim_dt)\n",
    "k1 = k1 * kscale\n",
    "plt.plot(kt, k1)\n",
    "\n",
    "# raised-cosine basis functions\n",
    "kcosbas = strf.cosbasis(ntau, ntbas)\n",
    "ntbas = kcosbas.shape[1]\n",
    "k1c = strf.to_basis(k1, kcosbas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the stimulus and response\n",
    "\n",
    "We are also using gaussian white noise here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generating spikes with biocm\n",
    "# this needs to be adjusted on a per model basis. posp ~ 2.0; phasic ~ 10\n",
    "# model_name = \"biocm_phasic\"\n",
    "# current_scaling = 9.0\n",
    "# model_name = \"biocm_tonic\"\n",
    "# current_scaling = 4.0\n",
    "model_name = \"pospischil_sm\"\n",
    "current_scaling = 2.0\n",
    "\n",
    "trial_noise_sd = 2.0\n",
    "spike_thresh = -20\n",
    "dt_rise_time = int(1.0 / model_dt)\n",
    "modelpath = \"../../models\"\n",
    "pymodel = spkc.load_model(os.path.join(modelpath, model_name + \".yml\"))\n",
    "biocm_params = spkc.to_array(pymodel[\"parameters\"])\n",
    "biocm_state0 = spkc.to_array(pymodel[\"state\"])\n",
    "biocm_model = spkc.load_module(pymodel, modelpath)\n",
    "    \n",
    "def filter_stimulus(S, k1):\n",
    "    return np.convolve(S, k1, mode=\"full\")[:S.size]\n",
    "\n",
    "def generate_spikes(I, noise_sd, dt, upsample):\n",
    "    from scipy.signal import resample\n",
    "    I_noise = np.random.randn(I.size) * noise_sd\n",
    "    I = current_scaling * (I + I_noise)\n",
    "    #I_resamp = sps.resample(I + I_noise, I.size * upsample)\n",
    "    X = biocm_model.integrate(biocm_params, biocm_state0, I, stim_dt, model_dt)\n",
    "    det = qs.detector(spike_thresh, dt_rise_time)\n",
    "    return I, X[:, 0], det(X[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data parameters\n",
    "duration = 100000\n",
    "n_bins = int(duration / model_dt)\n",
    "n_frames = n_bins // upsample\n",
    "n_assim = 10\n",
    "n_test = 10\n",
    "\n",
    "# generate data to fit\n",
    "np.random.seed(1)\n",
    "mat.random_seed(1)\n",
    "data = []\n",
    "stim = np.random.randn(n_frames)\n",
    "stim[:100] = 0\n",
    "        \n",
    "I = filter_stimulus(stim, k1)\n",
    "for i in range(n_assim):\n",
    "    In, V, spike_t = generate_spikes(I, trial_noise_sd, model_dt, upsample)\n",
    "    spike_v = np.zeros(V.size, 'i')\n",
    "    spike_v[spike_t] = 1\n",
    "    H = mat.adaptation(spike_v, ataus, model_dt)\n",
    "    d = {\"H\": H,\n",
    "         \"duration\": duration,\n",
    "         \"spike_t\": np.asarray(spike_t), \n",
    "         \"spike_v\": spike_v,\n",
    "        }\n",
    "    data.append(d)\n",
    "\n",
    "# split into assimilation and test sets\n",
    "assim_data = data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(9, 4))\n",
    "t_stim = np.linspace(0, duration, stim.size)\n",
    "t_voltage = np.linspace(0, duration, V.size)\n",
    "axes[0].set_title(\"Simulated {} response\".format(model_name))\n",
    "axes[0].plot(t_stim, stim)\n",
    "axes[1].plot(t_stim, I, t_voltage, V)\n",
    "for i, d in enumerate(data):\n",
    "    axes[2].vlines(d[\"spike_t\"] * model_dt, i, i + 0.5)\n",
    "for ax in axes:\n",
    "    ax.set_xlim(0, 8000)\n",
    "print(\"spikes: {}; rate: {} / dt\".format(np.mean([d[\"spike_t\"].size for d in data]), \n",
    "                                         np.mean([d[\"spike_t\"].size / d[\"duration\"] for d in data])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation\n",
    "\n",
    "The theano code for setting up maximum likelihood estimation has been factored out into the `mle` package.\n",
    "\n",
    "**Note**: The following cell sometimes generates an error the first time you run it. Just run it again if that happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initial guess of parameters using regularized ML\n",
    "ntbas = 8\n",
    "kcosbas = strf.cosbasis(ntau, ntbas)\n",
    "spike_v = np.stack([d[\"spike_v\"] for d in assim_data], axis=1)\n",
    "spike_h = np.stack([d[\"H\"] for d in assim_data], axis=2)\n",
    "mlest = mle.mat(stim, kcosbas, spike_v, spike_h, stim_dt, model_dt, nlin=\"exp\")\n",
    "%time w0 = mlest.estimate(reg_alpha=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"MLE rate and adaptation parameters:\", w0[:3])\n",
    "rf_ml = strf.from_basis(w0[3:], kcosbas)[::-1]\n",
    "plt.plot(k1, label=\"kernel\")\n",
    "\n",
    "# there is an expected shift due to the filtering properties of the membrane\n",
    "km, kmt = filters.exponential(46, 1.0, ntau * stim_dt, stim_dt)\n",
    "kconv = np.convolve(km, k1, mode=\"full\")[:km.size]\n",
    "kconv *= k1.max() / kconv.max()\n",
    "plt.plot(kconv, label=\"expected\")\n",
    "\n",
    "plt.plot(rf_ml * k1.max() / rf_ml.max(), label=\"MLE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior sampling\n",
    "\n",
    "The following cells sample from the posterior distribution of the parameters. This is useful if you want to do inference or generate plots showing the posteriors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# estimate parameters using emcee\n",
    "from neurofit import priors, costs, utils, startpos\n",
    "import emcee\n",
    "\n",
    "# assimilation parameters\n",
    "if sys.platform == 'darwin':\n",
    "    nthreads = 1\n",
    "else:\n",
    "    nthreads = 8\n",
    "nwalkers = 500\n",
    "nsteps = 200\n",
    "\n",
    "mat_prior = priors.joint_independent(\n",
    "                [ priors.uniform( 0,  20),\n",
    "                  priors.uniform(-50,  200),\n",
    "                  priors.uniform(-5,   10),\n",
    "                ])\n",
    "\n",
    "# lasso prior on RF parameters\n",
    "rf_lambda = 1.0\n",
    "\n",
    "# set boundaries on adaptation parameters based on disallowed region (see Yamauchi et al 2011)\n",
    "def matbounds(t1, t2, tr):\n",
    "    aa1 = -(1 - np.exp(-tr/t2))/(1 - np.exp(-tr/t1))\n",
    "    aa2 = -(np.exp(tr/t2) - 1)/(np.exp(tr/t1) - 1)\n",
    "    def f(mparams):\n",
    "        return (mparams[2] > aa1 * mparams[1]) and (mparams[2] > aa2 * mparams[1])\n",
    "    return f\n",
    "\n",
    "# refractory period - used to set bounds for a1 and a2\n",
    "t_refract = 2.0\n",
    "matboundprior = matbounds(ataus[0], ataus[1], t_refract)\n",
    "\n",
    "def lnprior(theta):\n",
    "    mparams = theta[:3]\n",
    "    rfparams = theta[3:]\n",
    "    if not matboundprior(mparams):\n",
    "        return -np.inf\n",
    "    rf_prior = -np.sum(np.abs(rfparams)) * rf_lambda\n",
    "    ll = mat_prior(mparams) + rf_prior\n",
    "    if not np.isfinite(ll):\n",
    "        return -np.inf\n",
    "    else:\n",
    "        return ll\n",
    "\n",
    "\n",
    "def loglike_poisson(V, H, spike_t, alpha, dt):\n",
    "    mu = V - np.dot(H, alpha)\n",
    "    return mu[spike_t].sum() - np.exp(mu).sum() * dt \n",
    "\n",
    "\n",
    "def loglike_sigmoid(V, H, spike_t, alpha, dt):\n",
    "    mu = V - np.dot(H, alpha)\n",
    "    lmb = (1 + np.tanh(mu / 2)) / 2\n",
    "    return np.log(lmb[spike_t]).sum() - lmb.sum() * dt \n",
    "\n",
    "\n",
    "def loglike_softplus(V, H, spike_t, alpha, dt):\n",
    "    mu = V - np.dot(H, alpha)\n",
    "    lmb = np.log1p(np.exp(mu))\n",
    "    return np.log(lmb[spike_t]).sum() - lmb.sum() * dt \n",
    "    \n",
    "    \n",
    "def lnlike(theta):\n",
    "    mparams = theta[:3]\n",
    "    rfparams = theta[3:]\n",
    "    Vi = mlest.V_interp(theta).squeeze() - mparams[0]\n",
    "    lp = 0\n",
    "    for d in assim_data:\n",
    "        lp += loglike_softplus(Vi, d[\"H\"], d[\"spike_t\"], mparams[1:3], model_dt)\n",
    "    return lp   \n",
    "\n",
    "def lnpost_dyn(theta):\n",
    "    \"\"\"Posterior probability for dynamical parameters\"\"\"\n",
    "    return lnprior(theta) - mlest.loglike(theta) #+ lnlike(theta) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initial state is a gaussian ball around the ML estimate\n",
    "p0 = startpos.normal_independent(nwalkers, w0, np.abs(w0) * 2)\n",
    "theta_0 = np.median(p0, 0)\n",
    "print(\"lnpost of p0 median: {}\".format(lnpost_dyn(theta_0)))\n",
    "%timeit lnpost_dyn(w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampler = emcee.EnsembleSampler(nwalkers, w0.size, lnpost_dyn, threads=nthreads)\n",
    "tracker = utils.convergence_tracker(nsteps, 25)\n",
    "\n",
    "for pos, prob, _ in tracker(sampler.sample(p0, iterations=nsteps)): \n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"lnpost of p median: {}\".format(np.median(prob)))\n",
    "print(\"average acceptance fraction: {}\".format(sampler.acceptance_fraction.mean()))\n",
    "theta = np.median(pos, 0)\n",
    "print(\"MLE:\", w0[:3])\n",
    "print(\"MAP:\", theta[:3])\n",
    "\n",
    "minprob = np.percentile(prob, 5)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 4))\n",
    "for i in range(nwalkers):\n",
    "    if prob[i] <= minprob: continue\n",
    "    k = strf.from_basis(pos[i,3:], kcosbas)[::-1]\n",
    "    axes[0].plot(k, 'k', alpha=0.01)\n",
    "\n",
    "kmap = strf.from_basis(theta[3:], kcosbas)[::-1]\n",
    "# expected kernel due to the filtering properties of the membrane\n",
    "axes[0].plot(kmap, 'y', label=\"MAP\")\n",
    "axes[0].plot(kconv * kmap.max() / kconv.max(), 'r', label=\"expected\")\n",
    "\n",
    "axes[0].set_title(model_name)\n",
    "axes[0].legend()\n",
    "\n",
    "# posterior distribution of post-spike kernels\n",
    "t = np.arange(0, 400)\n",
    "for i in range(nwalkers):\n",
    "    axes[1].plot(t, \n",
    "                 - pos[i,1] * np.exp(-t / ataus[0]) - pos[i,2] * np.exp(-t / ataus[1]),\n",
    "                 'k', alpha=0.01)\n",
    "axes[1].plot(t, - theta[1] * np.exp(-t / ataus[0]) - theta[2] * np.exp(-t / ataus[1]), 'y', label=\"MAP\")\n",
    "axes[1].legend()\n",
    "# save the results\n",
    "outfile = os.path.join(\"..\", \"..\", \"results\", \"1d_biocm\", model_name)\n",
    "plt.savefig(outfile + \"_rf.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ff in (kconv, kmap):\n",
    "    K = np.fft.fft(ff)\n",
    "    plt.plot(np.absolute(K[:K.size//2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from corner import corner\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "mpos = pos[:,:3]\n",
    "matlabs = ['a1','a2','w']\n",
    "c = corner(mpos,\n",
    "           #range=[sp for sp in startparams],\n",
    "       bins=50, smooth=2,smooth1d=0,\n",
    "       labels=matlabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# simulate test data\n",
    "def predict_spikes_current(I, params, dt, upsample):\n",
    "    state = mat.voltage(I, fullmatparams, dt, upsample=upsample)\n",
    "    V = state[:, 0]\n",
    "    return V, predict_spikes_voltage(V, params, dt, 1)\n",
    "\n",
    "def predict_spikes_voltage(V, params, dt, upsample):\n",
    "    omega, a1, a2  = params\n",
    "    return mat.predict_poisson(V - omega, (a1, a2), ataus, t_refract, dt, upsample)\n",
    "\n",
    "np.random.seed(1000)\n",
    "mat.random_seed(1000)\n",
    "test_data = []\n",
    "test_stim = np.random.randn(n_frames)\n",
    "test_stim[:100] = 0\n",
    "        \n",
    "I = filter_stimulus(test_stim, k1)\n",
    "for i in range(n_test):\n",
    "    In, V, spike_t = generate_spikes(I, trial_noise_sd, model_dt, upsample)\n",
    "    spike_v = np.zeros(V.size, 'i')\n",
    "    spike_v[spike_t] = 1\n",
    "    H = mat.adaptation(spike_v, ataus, model_dt)\n",
    "    d = {\"H\": H,\n",
    "         \"duration\": duration,\n",
    "         \"spike_t\": np.asarray(spike_t), \n",
    "         \"spike_v\": spike_v,\n",
    "        }\n",
    "    test_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dstrf import spikes, performance\n",
    "\n",
    "duration = 20000\n",
    "ax1 = plt.subplot(211)\n",
    "ax2 = plt.subplot(212)\n",
    "# posterior prediction\n",
    "spike_v = np.stack([d[\"spike_v\"] for d in test_data], axis=1)\n",
    "spike_h = np.stack([d[\"H\"] for d in test_data], axis=2)    \n",
    "mltest = mle.mat(test_stim, kcosbas, spike_v, spike_h, stim_dt, model_dt, nlin=\"exp\")\n",
    "\n",
    "samples = np.random.permutation(nwalkers)[:n_test]\n",
    "pred_spikes = []\n",
    "for i, idx in enumerate(samples):\n",
    "    sample = pos[idx]\n",
    "    V = mltest.V(sample)\n",
    "    S = predict_spikes_voltage(V, sample[:3], model_dt, upsample)\n",
    "    spk_t = S.nonzero()[0]\n",
    "    ax2.vlines(spk_t, i, i + 0.5, 'r')\n",
    "    pred_spikes.append(S)\n",
    "\n",
    "for j, d in enumerate(test_data):\n",
    "    ax2.vlines(d[\"spike_t\"], i + j + 1, i + j + 1.5)\n",
    "    \n",
    "test_psth = spikes.psth(spike_v, upsample, 1)    \n",
    "pred_psth = spikes.psth(np.asarray(pred_spikes).T,upsample,1)\n",
    "\n",
    "eo = performance.corrcoef(spike_v[::2], spike_v[1::2], upsample, 1)\n",
    "cc = np.corrcoef(test_psth, pred_psth)[0, 1]\n",
    "print(\"CC: {}/{} = {}\".format(cc, eo, cc / eo))\n",
    "\n",
    "ax1.plot(test_psth,linewidth=1,color='k',label=\"data\")\n",
    "ax1.plot(pred_psth,linewidth=1,color='r',label=\"estimate\")    \n",
    "t = ax1.text(duration,1.04,\"$R$: {:.2f}\".format(cc / eo),fontsize=10)\n",
    "t.set_ha(\"right\")\n",
    "\n",
    "ax1.set_xlim(0, duration // upsample)\n",
    "ax2.set_xlim(0, duration)\n",
    "\n",
    "plt.savefig(outfile + \"_predict.pdf\")\n",
    "np.savez(outfile + \"_samples.npz\", pos=pos, prob=prob, eo=eo, cc=cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "dstrf",
   "language": "python",
   "name": "dstrf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
