{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM Demo: univariate stimulus, MAT dynamics\n",
    "\n",
    "This is a testing notebook for the GLM estimation algorithm. We simulate data using the MAT model, and then try to estimate parameters with the GLM model. The main difference between these two models is that MAT has a membrane and GLMAT does not. This means that the estimated RF should be the convolution of the input kernel ($k1$) with the membrane kernel ($k2$) which is just an exponential decay with time constant $\\tau_m$. We should also be able to recover the mean rate ($\\omega$) and the two adaptation parameters ($\\alpha_1, \\alpha_2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import mat_neuron._model as mat\n",
    "from dstrf import strf, mle, filters\n",
    "\n",
    "# plotting packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # plotting functions\n",
    "import seaborn as sns           # data visualization package\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters: (\u03b11, \u03b12, \u03b2, \u03c9, R, \u03c4m, \u03c41, \u03c42, \u03c4V, tref)\n",
    "fullmatparams = np.asarray([100, 2, 0, 7, 1, 40, 10, 200, 5, 2], dtype='d')\n",
    "# reduced model parameters: (\u03c9, \u03b11, \u03b12, \u03c41, \u03c42, tref)\n",
    "matparams = fullmatparams[[3, 0, 1, 6, 7, 9]]\n",
    "model_dt = 0.5\n",
    "\n",
    "stim_dt = 10.0\n",
    "kscale = 1.0\n",
    "ntau = 60\n",
    "upsample = int(stim_dt / model_dt)\n",
    "# convolution kernel\n",
    "k1, kt = filters.gammadiff(ntau * stim_dt / 32, ntau * stim_dt / 16, 5 * kscale, ntau * stim_dt, stim_dt)\n",
    "# membrane kernel - note that the support is at model_dt intervals\n",
    "k2, k2t = filters.exponential(fullmatparams[5], fullmatparams[4], ntau * stim_dt, model_dt)\n",
    "plt.plot(kt, k1, kt, k2[::int(stim_dt / model_dt)])\n",
    "plt.xlabel(\"Time (ms)\")\n",
    "plt.title(\"Kernels\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the stimulus and response\n",
    "\n",
    "We're going to use Gaussian white noise for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stimulus(S, k1):\n",
    "    return np.convolve(S, k1, mode=\"full\")[:S.size]\n",
    "\n",
    "def predict_spikes_current(I, params, dt, upsample):\n",
    "    state = mat.voltage(I, fullmatparams, dt, upsample=upsample)\n",
    "    V = state[:, 0]\n",
    "    return V, predict_spikes_voltage(V, params, dt, 1)\n",
    "\n",
    "def predict_spikes_voltage(V, params, dt, upsample):\n",
    "    omega, a1, a2, t1, t2, tref = params\n",
    "    return mat.predict_poisson(V - omega, (a1, a2), (t1, t2), tref, dt, upsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data parameters\n",
    "duration = 100000\n",
    "n_bins = int(duration / model_dt)\n",
    "n_frames = n_bins // upsample\n",
    "n_assim = 1\n",
    "n_test = 5\n",
    "\n",
    "# generate data to fit\n",
    "np.random.seed(1)\n",
    "mat.random_seed(1)\n",
    "data = []\n",
    "stim = np.random.randn(n_frames)\n",
    "#stim[:100] = 0\n",
    "        \n",
    "I = filter_stimulus(stim, k1)\n",
    "for i in range(n_assim + n_test):\n",
    "    V, spikes = predict_spikes_current(I, matparams, model_dt, upsample)\n",
    "    H = mat.adaptation(spikes, matparams[3:5], model_dt)\n",
    "    z = np.nonzero(spikes)[0]\n",
    "    d = {\"H\": H,\n",
    "         \"duration\": duration,\n",
    "         \"spike_t\": z, \n",
    "         \"spike_v\": spikes,\n",
    "        }\n",
    "    data.append(d)\n",
    "\n",
    "# split into assimilation and test sets\n",
    "assim_data = data[:n_assim]\n",
    "test_data = data[n_assim:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=3, ncols=1, sharex=True, figsize=(9, 4))\n",
    "t_stim = np.linspace(0, duration, stim.size)\n",
    "t_voltage = np.linspace(0, duration, V.size)\n",
    "axes[0].set_title(\"Simulated MAT response\")\n",
    "axes[0].plot(t_stim, stim)\n",
    "axes[1].plot(t_stim, I, t_voltage, V)\n",
    "for i, d in enumerate(data):\n",
    "    axes[2].vlines(d[\"spike_t\"] * model_dt, i, i + 0.5)\n",
    "for ax in axes:\n",
    "    ax.set_xlim(0, 8000)\n",
    "print(\"spikes: {}; rate: {} / dt\".format(np.mean([d[\"spike_t\"].size for d in data]), \n",
    "                                         np.mean([d[\"spike_t\"].size / d[\"duration\"] for d in data])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation\n",
    "\n",
    "Start with using correlation and spike-triggered average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theano import config\n",
    "import scipy.optimize as op\n",
    "import imp\n",
    "imp.reload(strf)\n",
    "\n",
    "# cosine basis set\n",
    "kcosbas = strf.cosbasis(ntau, 10)\n",
    "kcosbas = ntau\n",
    "\n",
    "ftype = config.floatX\n",
    "# combine the trials\n",
    "spike_v = np.stack([d[\"spike_v\"] for d in assim_data], axis=1)\n",
    "# spikes in the exponential basis set\n",
    "X_spikes = np.stack([d[\"H\"] for d in assim_data], axis=2).astype(ftype)\n",
    "# generate design matrix for stimulus\n",
    "X_stim = strf.lagged_matrix(stim, kcosbas)\n",
    "# \"correct\" strf from current\n",
    "stx = np.dot(X_stim.T, I) / I.size / stim.var()\n",
    "# initial guess of strf from sta\n",
    "sta = strf.correlate(X_stim, spike_v)\n",
    "\n",
    "plt.plot(k1)\n",
    "plt.plot(strf.from_basis(stx, kcosbas)[::-1])\n",
    "plt.plot(strf.from_basis(sta, kcosbas)[::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do maximum likelihood estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theano import function, config, shared, sparse, gradient\n",
    "import theano.tensor as T\n",
    "from theano.tensor import nnet\n",
    "import scipy.sparse as sps\n",
    "# squash a bug on OS X\n",
    "config.gcc.cxxflags = \"-Wno-c++11-narrowing\"\n",
    "\n",
    "# the nonlinearity:\n",
    "nlin = T.exp\n",
    "\n",
    "if X_spikes.ndim == 2:\n",
    "    spike_design = np.expand_dims(X_spikes, 2)\n",
    "\n",
    "nframes, nk = X_stim.shape\n",
    "nbins, nalpha, ntrials = X_spikes.shape\n",
    "upsample = int(stim_dt / model_dt)\n",
    "# make an interpolation matrix\n",
    "interp = sps.kron(sps.eye(nframes),\n",
    "                  np.ones((upsample, 1), dtype=config.floatX),\n",
    "                  format='csc')\n",
    "\n",
    "\n",
    "# load the data into theano.shared structures\n",
    "M = shared(interp)\n",
    "dt = shared(model_dt)\n",
    "Xstim = shared(X_stim)\n",
    "Xspke = shared(np.rollaxis(X_spikes, 2))\n",
    "spikes = sps.csc_matrix(spike_v)\n",
    "Yspke = shared(spikes)\n",
    "\n",
    "# split out the parameter vector\n",
    "w = T.vector('w')\n",
    "dc = w[0]\n",
    "h = w[1:(nalpha+1)]\n",
    "k = w[(nalpha+1):]\n",
    "Vx = T.dot(Xstim, k)\n",
    "# Vx has to be promoted to a matrix for structured_dot to work\n",
    "Vi = sparse.structured_dot(M, T.shape_padright(Vx))\n",
    "H = T.dot(Xspke, h).T\n",
    "mu = Vi - H - dc\n",
    "lmb = nlin(mu)\n",
    "# this version of the log-likelihood is faster, but the gradient doesn't work\n",
    "llf = lmb.sum() * dt - sparse.sp_sum(sparse.structured_log(Yspke * lmb), sparse_grad=True)\n",
    "# this version has a working gradient\n",
    "ll = lmb.sum() * dt - sparse.sp_sum(Yspke * T.log(lmb), sparse_grad=True)\n",
    "# this is a penalty to keep the model out of the unallowed space\n",
    "penalty = - (h[0] + h[1] * matparams[3] / matparams[4])\n",
    "dL = T.grad(ll, w)\n",
    "# arbitrary vector for hessian-vector product\n",
    "v = T.vector('v')\n",
    "ddLv = T.grad(T.sum(dL * v), w)\n",
    "\n",
    "fV = function([w], Vx)\n",
    "fH = function([w], H)\n",
    "fL = function([w], llf)\n",
    "fgrad = function([w], dL)\n",
    "fhess = function([w, v], ddLv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial likelihood\n",
    "meanrate = spikes.sum(0).mean() / nbins\n",
    "w0 = np.r_[np.exp(meanrate), 0, 0, np.zeros_like(sta)]\n",
    "fL(w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w_ml = op.fmin_ncg(fL, w0, fgrad, fhess_p=fhess, avextol=1e-6, maxiter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"true rate and adaptation params:\", matparams[:3])\n",
    "print(\"MLE:\", w_ml[:3])\n",
    "plt.plot(k1, label=\"kernel\")\n",
    "# expected kernel is the convolution of k1 and k2\n",
    "kconv = np.convolve(k1, k2[::int(stim_dt / model_dt)], mode=\"full\")[:k1.size]\n",
    "plt.plot(kconv * k1.max() / kconv.max(), label=\"kernel * membrane\")\n",
    "plt.plot(strf.from_basis(w_ml[3:], kcosbas)[::-1], label=\"MLE\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posterior predictive distribution\n",
    "for j, d in enumerate(test_data):\n",
    "    plt.vlines(d[\"spike_t\"], j, j + 0.5, 'r')\n",
    "\n",
    "mparamp = matparams.copy()    \n",
    "for i in range(n_test):\n",
    "    V = fV(w_ml)\n",
    "    mparamp[:3] = w_ml[:3]\n",
    "    S = predict_spikes_voltage(V, mparamp, model_dt, upsample)\n",
    "    spk_t = S.nonzero()[0]\n",
    "    plt.vlines(spk_t, i + j + 1, i + j + 1.5)\n",
    "\n",
    "plt.xlim(0, 10000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "dstrf",
   "language": "python",
   "name": "dstrf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
