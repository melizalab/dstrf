{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from numba import jit\n",
    "\n",
    "from dstrf import mat, strf\n",
    "\n",
    "# plotting packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # plotting functions\n",
    "import seaborn as sns           # data visualization package\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters: (\u03b11, \u03b12, \u03c9, \u03c41, \u03c42, tref)\n",
    "matparams = np.asarray([100, 2, 7, 10, 200, 2], dtype='d')\n",
    "model_dt = 0.5\n",
    "\n",
    "# data parameters\n",
    "duration = 500000\n",
    "n_samples = int(duration / model_dt)\n",
    "n_assim = 1\n",
    "n_test = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution - simple alpha kernel\n",
    "from scipy.signal import resample\n",
    "stim_dt = 10.0\n",
    "upsample = int(stim_dt / model_dt)\n",
    "\n",
    "# alpha filter\n",
    "tau_h = 50\n",
    "tt = np.arange(0, 600, stim_dt)\n",
    "ka = np.flipud(tt / tau_h * np.exp(-tt / tau_h))\n",
    "\n",
    "# difference of gammas:\n",
    "from scipy.special import gamma\n",
    "tau_h1 = 600. / 32\n",
    "tau_h2 = 600. / 16\n",
    "kg1 = 1/(gamma(6)*tau_h1)*(tt/tau_h1)**5 * np.exp(-tt/tau_h1)\n",
    "kg2 = 1/(gamma(6)*tau_h2)*(tt/tau_h2)**5 * np.exp(-tt/tau_h2)\n",
    "kg = np.flipud(kg1 - kg2 / 1.5)\n",
    "kg /= np.linalg.norm(kg)\n",
    "\n",
    "# select a filter\n",
    "k1 = (kg * 5)[::-1]\n",
    "plt.plot(k1)\n",
    "\n",
    "def filter_stimulus(S, k1, upsample=None):\n",
    "    X = np.convolve(S, k1, mode=\"full\")[:S.size]\n",
    "    if upsample is not None:\n",
    "        t = np.arange(X.size)\n",
    "        tu = np.arange(0, X.size, 1 / upsample)\n",
    "        return np.interp(tu, t, X)\n",
    "    else:\n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def predict_spikes(V, params, dt, upsample):\n",
    "    N = V.size * upsample\n",
    "    S = np.zeros(N, dtype='i')\n",
    "    Vi = np.zeros(N, dtype='d')\n",
    "    R = np.random.uniform(size=N)\n",
    "    omega = params[2]\n",
    "    A1 = np.exp(-dt / params[3])\n",
    "    A2 = np.exp(-dt / params[4])\n",
    "    i_refrac = int(params[5] / dt)\n",
    "    H1 = H2 = 0\n",
    "    iref = 0\n",
    "    for i in range(N):\n",
    "        Vi[i] = V[i // upsample]\n",
    "        H1 *= A1\n",
    "        H2 *= A2\n",
    "        p = np.exp(Vi[i] - H1 - H2 - omega) * dt\n",
    "        if i > iref and p > R[i]:\n",
    "            H1 += params[0]\n",
    "            H2 += params[1]\n",
    "            iref = i + i_refrac\n",
    "            S[i] = 1\n",
    "    return Vi, S\n",
    "\n",
    "\n",
    "def lci_poisson(V, H, spikes, params, dt):\n",
    "    mu = V - H[:, 0] * params[0] - H[:, 1] * params[1] - params[2]\n",
    "    lp = spikes * mu - dt * np.exp(mu)\n",
    "    return lp.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data to fit\n",
    "np.random.seed(1)\n",
    "data = []\n",
    "stim = np.random.randn(int(n_samples / (stim_dt / model_dt)))\n",
    "stim[:100] = 0\n",
    "        \n",
    "V = filter_stimulus(stim, k1, upsample=1)\n",
    "for i in range(n_assim + n_test):\n",
    "    Vinterp, spikes = predict_spikes(V, matparams, model_dt, upsample)\n",
    "    H = np.c_[mat.predict_adaptation(spikes, matparams[3], model_dt),\n",
    "              mat.predict_adaptation(spikes, matparams[4], model_dt)]\n",
    "    z = np.nonzero(spikes)[0]\n",
    "    d = {\"V\": Vinterp,\n",
    "         \"H\": H,\n",
    "         \"duration\": duration,\n",
    "         \"spike_t\": z, \n",
    "         \"spike_v\": spikes,\n",
    "         \"lci\": lci_poisson(Vinterp, H, spikes, matparams, model_dt)\n",
    "         }\n",
    "    data.append(d)\n",
    "\n",
    "# split into assimilation and test sets\n",
    "assim_data = data[:n_assim]\n",
    "test_data = data[n_assim:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = plt.subplot(211)\n",
    "ax2 = plt.subplot(212)\n",
    "for i, d in enumerate(data):\n",
    "    ax2.vlines(d[\"spike_t\"], i, i + 0.5)\n",
    "ax1.plot(d[\"V\"])\n",
    "ax1.set_xlim(0, 8000)\n",
    "ax2.set_xlim(0, 8000)\n",
    "print(sum(d[\"lci\"] for d in data))\n",
    "print(\"spikes: {}; rate: {} / dt\".format(np.mean([d[\"spike_t\"].size for d in data]), \n",
    "                                         np.mean([d[\"spike_t\"].size / d[\"duration\"] for d in data])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial guess of strf\n",
    "# bin the spike train down to the stimulus rate\n",
    "psth = np.sum([np.sum(d[\"spike_v\"].reshape((int(n_samples / upsample), upsample)), axis=1) for d in data], axis=0)\n",
    "# generate design matrix for stimulus\n",
    "X_stim = strf.design_matrix(stim.reshape(1, stim.size), k1.size)\n",
    "w0 = np.dot(X_stim.T, psth) / np.sum(psth)\n",
    "rf_est = w0[::-1]\n",
    "plt.plot(rf_est)\n",
    "plt.plot(k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data for import into matlab\n",
    "import scipy.io as sio\n",
    "d = data[0]\n",
    "Itot = d[\"V\"] - d[\"H\"][:, 0] * matparams[0] - d[\"H\"][:, 1] * matparams[1] - matparams[2]\n",
    "sio.savemat(\"glmat_1d_data.mat\", {\"spikes\": d[\"spike_v\"],\n",
    "                                  \"spike_dt\": model_dt,\n",
    "                                  \"stim_dt\": stim_dt,\n",
    "                                  \"stim\": stim,\n",
    "                                  \"rf\": k1,\n",
    "                                  \"Itot\": Itot,\n",
    "                                  \"Istim\": d[\"V\"],\n",
    "                                  \"matparams\": matparams})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate cost function with matrix algebra\n",
    "import scipy.sparse as sps\n",
    "dc = matparams[2]\n",
    "alpha = matparams[:2]\n",
    "k = k1[::-1]\n",
    "w0 = np.r_[dc, alpha, k]\n",
    "spk = d[\"spike_t\"]\n",
    "interp = sps.kron(sps.eye(stim.size), np.ones((upsample, 1), dtype='i'), format='csc')\n",
    "interpsp = interp[spk,:]\n",
    "V = interp.dot(np.dot(X_stim, k)) - np.dot(H, alpha) - dc\n",
    "# log likelihood function\n",
    "rr = np.exp(V)\n",
    "ll = rr.sum() * model_dt - V[spk].sum()\n",
    "# gradient of ll:\n",
    "dLdk0 = np.dot(rr, interp.dot(X_stim))  # non-spiking term\n",
    "dLdb0 = rr.sum()\n",
    "dLda0 = np.dot(rr, H)\n",
    "dLdk1 = interpsp.dot(X_stim).sum(0)     # spiking term\n",
    "dLdb1 = spk.size\n",
    "dLda1 = H[spk].sum(0)\n",
    "gradL = np.r_[dLdb0 * model_dt - dLdb1, dLda0 * model_dt - dLda1, dLdk0 * model_dt - dLdk1]\n",
    "ll, gradL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do it with theano\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano import sparse\n",
    "\n",
    "M = theano.shared(interp)\n",
    "dt = theano.shared(model_dt)\n",
    "Xstim = theano.shared(X_stim)\n",
    "Xspke = theano.shared(H)\n",
    "sidx = theano.shared(spk)\n",
    "\n",
    "w = T.dvector('w')\n",
    "dc = w[0]\n",
    "alpha = w[1:3]\n",
    "k = w[3:]\n",
    "v = T.dvector('v')\n",
    "Vx = T.dot(Xstim, k)\n",
    "Hx = T.dot(Xspke, alpha)\n",
    "Vi = sparse.structured_dot(M, Vx.reshape((Vx.size, 1))).squeeze() - Hx - dc\n",
    "ll = T.exp(Vi).sum() * dt - Vi[sidx].sum() \n",
    "dL = T.grad(ll, w)\n",
    "ddL = theano.gradient.hessian(ll, w)\n",
    "\n",
    "th_Vi = theano.function([w], Vi)\n",
    "th_loglike = theano.function([w], ll, on_unused_input='warn')\n",
    "th_grad = theano.function([w], dL, on_unused_input='warn')\n",
    "th_hess = theano.function([w], ddL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(th_loglike(w0),\n",
    " th_grad(w0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as op\n",
    "w1 = op.fmin_ncg(th_loglike, w0, th_grad, fhess=th_hess, maxiter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w1[:3])\n",
    "rf_est = w1[:3:-1]\n",
    "plt.plot(rf_est)\n",
    "plt.plot(w0[:3:-1])\n",
    "plt.plot(k1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "dstrf",
   "language": "python",
   "name": "dstrf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}