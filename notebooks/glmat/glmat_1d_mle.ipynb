{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a testing notebook for the GLMAT estimation algorithm. We simulate data using the MAT model, and then try to estimate parameters with the GLMAT model. The difference between these two models is that MAT has a membrane and GLMAT does not. This means that the estimated RF should be the convolution of the input kernel ($k1$) with the membrane kernel ($k2$) which is just an exponential decay with time constant $\\tau_m$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import mat_neuron._model as mat\n",
    "from dstrf import strf, mle, filters\n",
    "\n",
    "# plotting packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # plotting functions\n",
    "import seaborn as sns           # data visualization package\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters: (\u03b11, \u03b12, \u03b2, \u03c9, R, \u03c4m, \u03c41, \u03c42, \u03c4V, tref)\n",
    "fullmatparams = np.asarray([100, 2, 0, 7, 1, 40, 10, 200, 5, 2], dtype='d')\n",
    "# reduced model parameters: (\u03c9, \u03b11, \u03b12, \u03c41, \u03c42, tref)\n",
    "matparams = fullmatparams[[3, 0, 1, 6, 7, 9]]\n",
    "# matparams = np.asarray([7, 100, 2, 10, 200, 2], dtype='d')\n",
    "#matparams = np.asarray([8, -2, 1, 10, 200, 2], dtype='d')\n",
    "# full model parameters: (\u03b11, \u03b12, \u03b2, \u03c9, \u03c4m, R, \u03c41, \u03c42, \u03c4V, tref)\n",
    "model_dt = 0.5\n",
    "\n",
    "stim_dt = 10.0\n",
    "kscale = 1.0\n",
    "ntau = 60\n",
    "upsample = int(stim_dt / model_dt)\n",
    "# membrane kernel\n",
    "k2, k2t = filters.exponential(fullmatparams[5], fullmatparams[4], ntau * stim_dt, model_dt)\n",
    "# convolution kernel\n",
    "k1, kt = filters.gammadiff(ntau * stim_dt / 32, ntau * stim_dt / 16, 5 * kscale, ntau * stim_dt, stim_dt)\n",
    "#k1, kt = filters.exponential(ntau * stim_dt / 16, 50 * kscale, ntau * stim_dt, stim_dt)\n",
    "plt.plot(kt, k1, kt, k2[::int(stim_dt / model_dt)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stimulus(S, k1):\n",
    "    return np.convolve(S, k1, mode=\"full\")[:S.size]\n",
    "\n",
    "def predict_spikes_current(I, params, dt, upsample):\n",
    "    state = mat.voltage(I, fullmatparams, dt, upsample=upsample)\n",
    "    V = state[:, 0]\n",
    "    return V, predict_spikes_voltage(V, params, dt, 1)\n",
    "\n",
    "def predict_spikes_voltage(V, params, dt, upsample):\n",
    "    omega, a1, a2, t1, t2, tref = params\n",
    "    return mat.predict_poisson(V - omega, (a1, a2), (t1, t2), tref, dt, upsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data parameters\n",
    "duration = 100000\n",
    "n_bins = int(duration / model_dt)\n",
    "n_frames = n_bins // upsample\n",
    "n_assim = 1\n",
    "n_test = 5\n",
    "\n",
    "# generate data to fit\n",
    "np.random.seed(1)\n",
    "mat.random_seed(1)\n",
    "data = []\n",
    "stim = np.random.randn(n_frames)\n",
    "#stim[:100] = 0\n",
    "        \n",
    "I = filter_stimulus(stim, k1)\n",
    "for i in range(n_assim + n_test):\n",
    "    V, spikes = predict_spikes_current(I, matparams, model_dt, upsample)\n",
    "    H = mat.adaptation(spikes, matparams[3:5], model_dt)\n",
    "    z = np.nonzero(spikes)[0]\n",
    "    d = {\"H\": H,\n",
    "         \"duration\": duration,\n",
    "         \"spike_t\": z, \n",
    "         \"spike_v\": spikes,\n",
    "        }\n",
    "    data.append(d)\n",
    "\n",
    "# split into assimilation and test sets\n",
    "assim_data = data[:n_assim]\n",
    "test_data = data[n_assim:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_stim = np.linspace(0, duration, stim.size)\n",
    "t_voltage = np.linspace(0, duration, V.size)\n",
    "plt.subplot(311).plot(t_stim, stim)\n",
    "plt.subplot(312).plot(t_stim, I, t_voltage, V)\n",
    "ax2 = plt.subplot(313)\n",
    "for i, d in enumerate(data):\n",
    "    ax2.vlines(d[\"spike_t\"] * model_dt, i, i + 0.5)\n",
    "for ax in plt.gcf().axes:\n",
    "    ax.set_xlim(0, 8000)\n",
    "print(\"spikes: {}; rate: {} / dt\".format(np.mean([d[\"spike_t\"].size for d in data]), \n",
    "                                         np.mean([d[\"spike_t\"].size / d[\"duration\"] for d in data])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theano import config\n",
    "import scipy.optimize as op\n",
    "\n",
    "# cosine basis set\n",
    "kcosbas = strf.cosbasis(ntau, 10)\n",
    "kcosbas = ntau\n",
    "\n",
    "ftype = config.floatX\n",
    "# combine the trials\n",
    "spike_v = np.stack([d[\"spike_v\"] for d in assim_data], axis=1)\n",
    "# spikes in the exponential basis set\n",
    "X_spikes = np.stack([d[\"H\"] for d in assim_data], axis=2).astype(ftype)\n",
    "# generate design matrix for stimulus\n",
    "X_stim = strf.lagged_matrix(stim, kcosbas)\n",
    "# \"correct\" strf from current\n",
    "stx = np.dot(X_stim.T, I) / I.size / stim.var()\n",
    "# initial guess of strf from sta\n",
    "sta = strf.correlate(X_stim, spike_v)\n",
    "\n",
    "plt.plot(k1)\n",
    "plt.plot(strf.from_basis(stx, kcosbas)[::-1])\n",
    "plt.plot(strf.from_basis(sta, kcosbas)[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theano import function, config, shared, sparse, gradient\n",
    "import theano.tensor as T\n",
    "from theano.tensor import nnet\n",
    "import scipy.sparse as sps\n",
    "\n",
    "# the nonlinearity:\n",
    "nlin = T.exp\n",
    "\n",
    "if X_spikes.ndim == 2:\n",
    "    spike_design = np.expand_dims(X_spikes, 2)\n",
    "\n",
    "nframes, nk = X_stim.shape\n",
    "nbins, nalpha, ntrials = X_spikes.shape\n",
    "upsample = int(stim_dt / model_dt)\n",
    "# make an interpolation matrix\n",
    "interp = sps.kron(sps.eye(nframes),\n",
    "                  np.ones((upsample, 1), dtype=config.floatX),\n",
    "                  format='csc')\n",
    "\n",
    "\n",
    "# load the data into theano.shared structures\n",
    "M = shared(interp)\n",
    "dt = shared(model_dt)\n",
    "Xstim = shared(X_stim)\n",
    "Xspke = shared(np.rollaxis(X_spikes, 2))\n",
    "spikes = sps.csc_matrix(spike_v)\n",
    "Yspke = shared(spikes)\n",
    "\n",
    "# split out the parameter vector\n",
    "w = T.vector('w')\n",
    "dc = w[0]\n",
    "h = w[1:(nalpha+1)]\n",
    "k = w[(nalpha+1):]\n",
    "Vx = T.dot(Xstim, k)\n",
    "# Vx has to be promoted to a matrix for structured_dot to work\n",
    "Vi = sparse.structured_dot(M, T.shape_padright(Vx))\n",
    "H = T.dot(Xspke, h).T\n",
    "mu = Vi - H - dc\n",
    "lmb = nlin(mu)\n",
    "# this version of the log-likelihood is faster, but the gradient doesn't work\n",
    "llf = lmb.sum() * dt - sparse.sp_sum(sparse.structured_log(Yspke * lmb), sparse_grad=True)\n",
    "# this version has a working gradient\n",
    "ll = lmb.sum() * dt - sparse.sp_sum(Yspke * T.log(lmb), sparse_grad=True)\n",
    "# this is a penalty to keep the model out of the unallowed space\n",
    "penalty = - (h[0] + h[1] * matparams[3] / matparams[4])\n",
    "dL = T.grad(ll, w)\n",
    "# arbitrary vector for hessian-vector product\n",
    "v = T.vector('v')\n",
    "ddLv = T.grad(T.sum(dL * v), w)\n",
    "\n",
    "fV = function([w], Vx)\n",
    "fH = function([w], H)\n",
    "fL = function([w], llf)\n",
    "fgrad = function([w], dL)\n",
    "fhess = function([w, v], ddLv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial likelihood\n",
    "meanrate = spikes.sum(0).mean() / nbins\n",
    "w0 = np.r_[np.exp(meanrate), 0, 0, np.zeros_like(sta)]\n",
    "fL(w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w_ml = op.fmin_ncg(fL, w0, fgrad, fhess_p=fhess, avextol=1e-6, maxiter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matparams[:3])\n",
    "print(w_ml[:3])\n",
    "plt.plot(k1)\n",
    "# expected kernel is the convolution of k1 and k2\n",
    "kconv = np.convolve(k1, k2[::int(stim_dt / model_dt)], mode=\"full\")[:k1.size]\n",
    "plt.plot(kconv * k1.max() / kconv.max())\n",
    "plt.plot(strf.from_basis(w_ml[3:], kcosbas)[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posterior predictive distribution\n",
    "for j, d in enumerate(test_data):\n",
    "    plt.vlines(d[\"spike_t\"], j, j + 0.5, 'r')\n",
    "\n",
    "mparamp = matparams.copy()    \n",
    "for i in range(n_test):\n",
    "    V = fV(w_ml)\n",
    "    mparamp[:3] = w_ml[:3]\n",
    "    S = predict_spikes_voltage(V, mparamp, model_dt, upsample)\n",
    "    spk_t = S.nonzero()[0]\n",
    "    plt.vlines(spk_t, i + j + 1, i + j + 1.5)\n",
    "\n",
    "plt.xlim(0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit to a subset of the data. Need to update shared variables.\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=4)\n",
    "i_frames = np.arange(nframes)\n",
    "i_bins = np.arange(nbins)\n",
    "ftrain, ftest = next(kf.split(i_frames))\n",
    "strain, stest = next(kf.split(i_bins))\n",
    "\n",
    "M.set_value(interp[strain][:, ftrain])\n",
    "Xstim.set_value(X_stim[ftrain])\n",
    "Xspke.set_value(np.rollaxis(X_spikes[strain], 2))\n",
    "Yspke.set_value(spikes[strain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w_ml = op.fmin_ncg(fL, w0, fgrad, fhess_p=fhess, avextol=1e-6, maxiter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.set_value(interp[stest][:, ftest])\n",
    "Xstim.set_value(X_stim[ftest])\n",
    "Xspke.set_value(np.rollaxis(X_spikes[ftest], 2))\n",
    "Yspke.set_value(spikes[ftest])"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "dstrf",
   "language": "python",
   "name": "dstrf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}