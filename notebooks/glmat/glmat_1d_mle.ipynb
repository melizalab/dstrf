{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import mat_neuron._model as mat\n",
    "from dstrf import strf, mle\n",
    "\n",
    "# plotting packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # plotting functions\n",
    "import seaborn as sns           # data visualization package\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters: (\u03c9, \u03b11, \u03b12, \u03c41, \u03c42, tref)\n",
    "matparams = np.asarray([7, 100, 2, 10, 200, 2], dtype='d')\n",
    "model_dt = 0.5\n",
    "\n",
    "# convolution kernel\n",
    "from dstrf import filters\n",
    "stim_dt = 10.0\n",
    "kscale = 1.0\n",
    "ntau = 60\n",
    "upsample = int(stim_dt / model_dt)\n",
    "k1, kt = filters.gammadiff(ntau * stim_dt / 32, ntau * stim_dt / 16, 5, ntau * stim_dt, stim_dt)\n",
    "k1 = k1[::-1] * kscale\n",
    "plt.plot(-kt[::-1], k1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stimulus(S, k1):\n",
    "    return np.correlate(S, k1, mode=\"full\")[:S.size]\n",
    "\n",
    "def predict_spikes(V, params, dt, upsample):\n",
    "    omega, a1, a2, t1, t2, tref = params\n",
    "    return mat.predict_poisson(V - omega, (a1, a2), (t1, t2), tref, \n",
    "                               dt, upsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data parameters\n",
    "duration = 100000\n",
    "n_samples = int(duration / model_dt)\n",
    "n_assim = 2\n",
    "n_test = 5\n",
    "\n",
    "# generate data to fit\n",
    "np.random.seed(1)\n",
    "mat.random_seed(1)\n",
    "data = []\n",
    "stim = np.random.randn(n_samples // upsample)\n",
    "stim[:100] = 0\n",
    "        \n",
    "V = filter_stimulus(stim, k1)\n",
    "for i in range(n_assim + n_test):\n",
    "    spikes = predict_spikes(V, matparams, model_dt, upsample)\n",
    "    H = mat.adaptation(spikes, matparams[3:5], model_dt)\n",
    "    z = np.nonzero(spikes)[0]\n",
    "    d = {\"H\": H,\n",
    "         \"duration\": duration,\n",
    "         \"spike_t\": z, \n",
    "         \"spike_v\": spikes,\n",
    "        }\n",
    "    data.append(d)\n",
    "\n",
    "# split into assimilation and test sets\n",
    "assim_data = data[:n_assim]\n",
    "test_data = data[n_assim:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = plt.subplot(211)\n",
    "ax2 = plt.subplot(212)\n",
    "for i, d in enumerate(data):\n",
    "    ax2.vlines(d[\"spike_t\"], i, i + 0.5)\n",
    "ax1.plot(V)\n",
    "ax1.set_xlim(0, 8000 // upsample)\n",
    "ax2.set_xlim(0, 8000)\n",
    "print(\"spikes: {}; rate: {} / dt\".format(np.mean([d[\"spike_t\"].size for d in data]), \n",
    "                                         np.mean([d[\"spike_t\"].size / d[\"duration\"] for d in data])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theano import config\n",
    "import scipy.optimize as op\n",
    "\n",
    "# cosine basis set\n",
    "kcosbas = strf.cosbasis(ntau, 8, lin=10)\n",
    "\n",
    "ftype = config.floatX\n",
    "# combine the trials\n",
    "spikes = np.stack([d[\"spike_v\"] for d in assim_data], axis=1).astype(ftype)\n",
    "# spikes in the exponential basis set\n",
    "X_spikes = np.stack([d[\"H\"] for d in data], axis=2).astype(ftype)\n",
    "# generate design matrix for stimulus\n",
    "X_stim = strf.lagged_matrix(stim, kcosbas)\n",
    "# initial guess of strf\n",
    "sta = strf.correlate(X_stim, spikes)\n",
    "plt.plot(k1)\n",
    "plt.plot(strf.from_basis(sta, kcosbas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from theano import function, config, shared, sparse, gradient\n",
    "import theano.tensor as T\n",
    "from theano.tensor import nnet\n",
    "import scipy.sparse as sps\n",
    "\n",
    "# the nonlinearity:\n",
    "nlin = T.exp\n",
    "\n",
    "if X_spikes.ndim == 2:\n",
    "    spike_design = np.expand_dims(X_spikes, 2)\n",
    "if spikes.ndim == 1:\n",
    "    spikes = np.expand_dims(spikes, 1)\n",
    "\n",
    "nframes, nk = X_stim.shape\n",
    "nbins, nalpha, ntrials = X_spikes.shape\n",
    "upsample = int(stim_dt / model_dt)\n",
    "# make an interpolation matrix\n",
    "interp = sps.kron(sps.eye(nframes),\n",
    "                  np.ones((upsample, 1), dtype=config.floatX),\n",
    "                  format='csc')\n",
    "\n",
    "# load the data into theano.shared structures\n",
    "M = shared(interp)\n",
    "dt = shared(model_dt)\n",
    "Xstim = shared(X_stim)\n",
    "Xspke = shared(np.rollaxis(X_spikes, 2))\n",
    "spkx, spky = map(shared, spikes.nonzero())\n",
    "\n",
    "# split out the parameter vector\n",
    "w = T.vector('w')\n",
    "dc = w[0]\n",
    "h = w[1:(nalpha+1)]\n",
    "k = w[(nalpha+1):]\n",
    "Vx = T.dot(Xstim, k)\n",
    "# Vx has to be promoted to a matrix for structured_dot to work\n",
    "Vi = sparse.structured_dot(M, T.shape_padright(Vx))\n",
    "H = T.dot(Xspke, h).T\n",
    "mu = Vi - H - dc\n",
    "lmb = nlin(mu)\n",
    "ll = lmb.sum() * dt - T.log(lmb[spkx, spky]).sum()\n",
    "dL = T.grad(ll, w)\n",
    "# arbitrary vector for hessian-vector product\n",
    "v = T.vector('v')\n",
    "ddLv = T.grad(T.sum(dL * v), w)\n",
    "\n",
    "fV = function([w], Vx)\n",
    "fH = function([w], H)\n",
    "fL = function([w], ll)\n",
    "fgrad = function([w], dL)\n",
    "fhess = function([w, v], ddLv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial likelihood\n",
    "w0 = np.r_[0, 0, 0, sta]\n",
    "fL(w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_ml = op.fmin_ncg(fL, w0, fgrad, fhess_p=fhess, avextol=1e-6, maxiter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w_ml[:3])\n",
    "plt.plot(k1)\n",
    "plt.plot(strf.from_basis(w_ml[3:], kcosbas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posterior predictive distribution\n",
    "for j, d in enumerate(test_data):\n",
    "    plt.vlines(d[\"spike_t\"], j, j + 0.5, 'r')\n",
    "\n",
    "mparamp = matparams.copy()    \n",
    "for i in range(n_test):\n",
    "    V = fV(w_ml)\n",
    "    mparamp[:3] = w_ml[:3]\n",
    "    S = predict_spikes(V, mparamp, model_dt, upsample)\n",
    "    spk_t = S.nonzero()[0]\n",
    "    plt.vlines(spk_t, i + j + 1, i + j + 1.5)\n",
    "\n",
    "plt.xlim(0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "dstrf",
   "language": "python",
   "name": "dstrf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}