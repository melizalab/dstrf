{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLMAT: 2D kernel, song stimuli, ML+MC estimation\n",
    "\n",
    "This notebook demonstrates the full assimilation technique using song stimuli. The song waveform is processed to a 2D spectrogram, then convolved with a 2D STRF to produce the \"voltage\" of the GLMAT model. The adaptation \"current\" is calculated by convolving the spike trains with two exponential kernels. The goal of the assimilation is to estimate the parameters of the RF and the adaptation kernels. The parameter count of the RF is minimized by using a low-rank approximation (i.e., an outer product of two vectors) and by projecting time into a basis set of raised cosine filters that are spaced exponentially.\n",
    "\n",
    "The approach is to use elastic-net penalized maximum-likelihood estimation to get a first guess at the parameters. The regularization parameters and rank are selected using cross-validation. Then MCMC is used to sample the posterior distribution of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import sys\n",
    "import imp\n",
    "import numpy as np\n",
    "\n",
    "import yaml\n",
    "\n",
    "import mat_neuron._model as mat\n",
    "from dstrf import strf, mle, io, performance, spikes\n",
    "\n",
    "cell = sys.argv[1]\n",
    "yfile = sys.argv[2]\n",
    "saveplace = sys.argv[3]\n",
    "tag = sys.argv[4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MAT model is governed by a small number of parameters: the spike threshold (omega), the amplitudes of the adaptation kernels (alpha_1, alpha_2), the time constants of the adaptation kernels (tau_1, tau_2), and the absolute refactory period. In addition, a function must be chosen for spike generation. The 'softplus' function, log(1 + exp(mu)), is a good choice because it doesn't saturate as readily when mu is large. Because there can only be one spike per bin, saturation causes the estimated parameters to be less than the true parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(yfile,\"r\") as yf:\n",
    "    config = yaml.load(yf)\n",
    "    \n",
    "# set variables based on `config`\n",
    "ntaus = len(config[\"mat\"][\"taus\"])\n",
    "mat_fixed = np.asanyarray(config[\"mat\"][\"taus\"] + [config[\"mat\"][\"refract\"]],dtype='d')\n",
    "upsample = int(config[\"strf\"][\"stim_dt\"] / config[\"mat\"][\"model_dt\"])\n",
    "kcosbas = strf.cosbasis(config[\"strf\"][\"ntau\"], config[\"strf\"][\"ntbas\"])\n",
    "ntbas = kcosbas.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load some data from a real neural recording from the CRCNS dataset. To simplify the model, we concatenate the stimuli, setting padding between the stimuli sufficient to capture any offset responses. Note that the spike responses are convolved with the adaptation kernels before merging stimuli so that we don't inadvertently carry over spike history from trials that are not truly contiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_after = config[\"strf\"][\"ntau\"] * config[\"strf\"][\"stim_dt\"] # how much to pad after offset\n",
    "\n",
    "data = io.load_crcns(cell, config[\"data\"][\"stim_type\"], config[\"data\"][\"root\"], \n",
    "                     config[\"strf\"][\"spec_window\"], config[\"strf\"][\"stim_dt\"], \n",
    "                     f_min=config[\"strf\"][\"f_min\"], \n",
    "                     f_max=config[\"strf\"][\"f_max\"], f_count=config[\"strf\"][\"nfreq\"], \n",
    "                     compress=config[\"strf\"][\"spec_compress\"], \n",
    "                     gammatone=config[\"strf\"][\"gammatone\"])\n",
    "io.pad_stimuli(data, config[\"data\"][\"pad_before\"], pad_after, fill_value=0.0)\n",
    "io.preprocess_spikes(data, config[\"mat\"][\"model_dt\"], config[\"mat\"][\"taus\"])\n",
    "\n",
    "n_test = int(config[\"data\"][\"p_test\"] * len(data))\n",
    "\n",
    "# split into assimilation and test sets and merge stimuli\n",
    "assim_data = io.merge_data(data[:-n_test])\n",
    "test_data = io.merge_data(data[-n_test:])\n",
    "stim = assim_data[\"stim\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate parameters\n",
    "\n",
    "The reg_alpha and reg_lambda parameters set the L1 and L2 penalties for the initial ML estimation. Note that we supply the nonlinearity function to the constructor too, as this determines how the log-likelihood is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 12293.731292\n",
      "         Iterations: 34\n",
      "         Function evaluations: 40\n",
      "         Gradient evaluations: 73\n",
      "         Hessian evaluations: 306\n"
     ]
    }
   ],
   "source": [
    "# initial guess of parameters using penalized ML. Note that we provide the cosine basis set to the constructor of\n",
    "# mle.estimator, which causes the design matrix to be in the cosine basis set\n",
    "# we'll do an initial fit with some strong regularization to see if there's an RF. Leave this out of production\n",
    "# NB: This cell sometimes fails in initializing the estimator; just run it again.\n",
    "mlest = mle.mat(assim_data[\"stim\"], kcosbas, assim_data[\"spike_v\"], assim_data[\"spike_h\"],\n",
    "                assim_data[\"stim_dt\"], assim_data[\"spike_dt\"], nlin=config[\"mat\"][\"nlin\"])\n",
    "w0 = mlest.estimate(reg_lambda=1e1, reg_alpha=1e1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularization parameters (L1/L2 ratio and total penalty) are chosen using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " [Elapsed Time: 0:15:46] |#################################################################################################################################| (ETA:  0:00:00) "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best solution: rank=1.000, alpha=14.614, lambda=1.624, loglike=-12482.400\n",
      "[ 4.52810297 -0.14853789  0.08720627]\n"
     ]
    }
   ],
   "source": [
    "import progressbar\n",
    "from dstrf import crossvalidate\n",
    "\n",
    "#reg_grid = np.logspace(-1, 5, 50)[::-1]\n",
    "l1_ratios = [0.1, 0.5, 0.9] #[0.1, 0.5, 0.7, 0.9, 0.95]\n",
    "\n",
    "l1_ratios = [0.9]\n",
    "reg_grid = np.logspace(-1, 5, 20)[::-1]\n",
    "\n",
    "bar = progressbar.ProgressBar(max_value=2 * len(l1_ratios) * len(reg_grid),\n",
    "                              widgets=[\n",
    "                                ' [', progressbar.Timer(), '] ',\n",
    "                                progressbar.Bar(),\n",
    "                                ' (', progressbar.ETA(), ') ',\n",
    "                            ])\n",
    "\n",
    "i = 0\n",
    "scores = []\n",
    "results = []\n",
    "for krank in (1, 2):\n",
    "    mlest = mle.matfact(assim_data[\"stim\"], kcosbas, krank, assim_data[\"spike_v\"], assim_data[\"spike_h\"],\n",
    "                        assim_data[\"stim_dt\"], assim_data[\"spike_dt\"], nlin=config[\"mat\"][\"nlin\"])\n",
    "    for reg, s, w in crossvalidate.elasticnet(mlest, 4, reg_grid, l1_ratios, avextol=1e-5, disp=False):\n",
    "        i += 1\n",
    "        bar.update(i)\n",
    "        scores.append(s)\n",
    "        results.append((reg, krank, s, w))\n",
    "    \n",
    "best_idx = np.argmax(scores)\n",
    "best = results[best_idx]\n",
    "\n",
    "krank = best[1]\n",
    "rf_alpha, rf_lambda = best[0]\n",
    "w0 = best[3]\n",
    "print(\"Elastic Net Penalized Maximum Likelihood:\")\n",
    "print(\"best solution: rank={:.3f}, alpha={:.3f}, lambda={:.3f}, loglike={:.3f}\".format(krank, rf_alpha, rf_lambda, best[2]))\n",
    "print(w0[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlest = mle.matfact(assim_data[\"stim\"], kcosbas, krank, assim_data[\"spike_v\"], assim_data[\"spike_h\"],\n",
    "                        assim_data[\"stim_dt\"], assim_data[\"spike_dt\"], \n",
    "                        nlin=config[\"mat\"][\"nlin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loglike: -2832.299\n",
      "CC: 0.479 / 0.654 (0.733)\n",
      "spike count: data = 52.7, pred = 57.7\n"
     ]
    }
   ],
   "source": [
    "mltest = mle.matfact(test_data[\"stim\"], kcosbas, krank, test_data[\"spike_v\"], test_data[\"spike_h\"],\n",
    "                     test_data[\"stim_dt\"], test_data[\"spike_dt\"], nlin=config[\"mat\"][\"nlin\"])\n",
    "\n",
    "n_ppost = 10\n",
    "mat.random_seed(1)\n",
    "V = mltest.V(w0)\n",
    "pred = np.zeros_like(test_data[\"spike_v\"])\n",
    "for i in range(n_ppost):\n",
    "    pred[:, i] = mltest.predict(w0, mat_fixed, V)\n",
    "pred_psth = spikes.psth(pred, upsample, 1)\n",
    "test_psth = spikes.psth(test_data[\"spike_v\"], upsample, 1)\n",
    "\n",
    "psth_corr = np.corrcoef(test_psth, pred_psth)[0, 1]\n",
    "eo = performance.corrcoef(test_data[\"spike_v\"][::2], test_data[\"spike_v\"][1::2], upsample, 1)\n",
    "print(\"loglike: {:.3f}\".format(-mltest.loglike(w0)))\n",
    "print(\"CC: {:.3f} / {:.3f} ({:.3f})\".format(psth_corr, eo, psth_corr/eo))\n",
    "print(\"spike count: data = {}, pred = {}\".format(test_data[\"spike_v\"].sum() / config[\"data\"][\"n_trials\"], pred.sum() / n_ppost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the ML estimate to seed the MCMC sampler. We're going to reduce the size of the parameter space by factorizing the RF (i.e., a bilinear approximation). Note that we try to use the mlest object as much as possible to do the calculations rather than reimplement things; however, there can be some significant performance enhancements from an optimized implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate parameters using emcee\n",
    "from neurofit import priors, costs, utils, startpos\n",
    "\n",
    "# the MAT parameters are just bounded between reasonable limits. These may need to be expanded when using real data\"][\n",
    "mat_prior = priors.joint_independent(\n",
    "                [ priors.uniform(config[\"mat\"][\"bounds\"][0][0], config[\"mat\"][\"bounds\"][0][1]),\n",
    "                  priors.uniform(config[\"mat\"][\"bounds\"][1][0], config[\"mat\"][\"bounds\"][1][1]),\n",
    "                  priors.uniform(config[\"mat\"][\"bounds\"][2][0], config[\"mat\"][\"bounds\"][2][1]),\n",
    "                ])\n",
    "\n",
    "# use the regularization parameters from the cross-validation\n",
    "rf_alpha, rf_lambda = best[0]\n",
    "\n",
    "def lnpost(theta):\n",
    "    \"\"\"Posterior probability for dynamical parameters\"\"\"\n",
    "    mparams = theta[:3]\n",
    "    rfparams = theta[3:]\n",
    "    ll = mat_prior(mparams)\n",
    "    if not np.isfinite(ll):\n",
    "        return -np.inf\n",
    "    w = np.r_[mparams, rfparams]\n",
    "    ll -= mlest.loglike(w, rf_lambda, rf_alpha)\n",
    "    return -np.inf if not np.isfinite(ll) else ll\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code starts the MCMC sampler. We initialize the walkers (chains) in a gaussian around the ML estimate, with standard deviation 2x the absolute value of the best guess. The model converges fairly quickly, but then we let it sample for a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "if sys.platform == 'darwin':\n",
    "    config[\"emcee\"][\"nthreads\"] = 1\n",
    "\n",
    "# initialize walkers\n",
    "pos = p0 = startpos.normal_independent(config[\"emcee\"][\"nwalkers\"], w0, np.abs(w0) * 2)\n",
    "# initialize the sampler\n",
    "sampler = emcee.EnsembleSampler(config[\"emcee\"][\"nwalkers\"], w0.size, lnpost, \n",
    "                                threads=config[\"emcee\"][\"nthreads\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step  μ(lnprob)  Δ(lnprob)  σ(lnprob)        time         ETA\n",
      "---------------------------------------------------------------\n",
      "    25  -3.6486e+08        inf  1.152e+10     0:01:54     0:17:55\n",
      "    50     -12721  3.6485e+08       1213     0:03:52     0:15:54\n",
      "    75     -12577      143.7      841.3     0:05:51     0:13:56\n",
      "   100     -12565     12.641     830.18     0:07:49     0:11:55\n",
      "   125     -12557     7.1793     811.94     0:09:46     0:09:56\n",
      "   150     -12554     3.0415     806.11     0:11:45     0:07:58\n",
      "   175     -12553     1.6148     802.73     0:13:42     0:05:59\n",
      "   200     -12548     4.7392     786.66     0:15:40     0:04:01\n",
      "   225     -12542     6.0814     767.31     0:17:38     0:02:02\n",
      "   250     -12539     3.0422     754.84     0:19:36     0:00:04\n",
      "   251     -12539   0.018651     754.84     0:19:41     0:00:00\n"
     ]
    }
   ],
   "source": [
    "# start the sampler\n",
    "tracker = utils.convergence_tracker(config[\"emcee\"][\"nsteps\"], int(config[\"emcee\"][\"nsteps\"]/10.0))\n",
    "for pos, prob, like in tracker(sampler.sample(pos, iterations=config[\"emcee\"][\"nsteps\"], storechain=True)): \n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lnpost of p median: -12453.626300699225\n",
      "average acceptance fraction: 0.26314\n",
      "[ 4.54033839 -0.17606786  0.09235773]\n"
     ]
    }
   ],
   "source": [
    "print(\"lnpost of p median: {}\".format(np.median(prob)))\n",
    "print(\"average acceptance fraction: {}\".format(sampler.acceptance_fraction.mean()))\n",
    "try:\n",
    "    print(\"autocorrelation time: {}\".format(sampler.acor))\n",
    "except:\n",
    "    pass    \n",
    "w1 = np.median(pos, 0)\n",
    "rfparams = w1[3:]\n",
    "rf_map = strf.from_basis(mlest.strf(w0), kcosbas)\n",
    "print(w1[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loglike: -2838.027\n",
      "CC: 0.502 / 0.654 (0.768)\n",
      "spike count: data = 52.7, pred = 57.7\n"
     ]
    }
   ],
   "source": [
    "n_ppost = 10\n",
    "mat.random_seed(1)\n",
    "t_stim = np.linspace(0, test_data[\"duration\"], test_data[\"stim\"].shape[1])\n",
    "    \n",
    "samples = np.random.permutation(config[\"emcee\"][\"nwalkers\"])[:n_ppost]\n",
    "pred = np.zeros((test_data[\"spike_v\"].shape[0], n_ppost), dtype=test_data[\"spike_v\"].dtype)\n",
    "for i, idx in enumerate(samples):\n",
    "    mparams = pos[idx]\n",
    "    V_mc = mltest.V(mparams)\n",
    "    pred[:, i] = mltest.predict(mparams, mat_fixed, V_mc)\n",
    "    spk_t = pred[:, i].nonzero()[0]\n",
    "\n",
    "pred_psth = spikes.psth(pred, upsample, 1)\n",
    "test_psth = spikes.psth(test_data[\"spike_v\"], upsample, 1)\n",
    "\n",
    "psth_corr = np.corrcoef(test_psth, pred_psth)[0, 1]\n",
    "eo = performance.corrcoef(test_data[\"spike_v\"][::2], test_data[\"spike_v\"][1::2], upsample, 1)\n",
    "\n",
    "print(\"emcee:\")\n",
    "print(\"loglike: {:.3f}\".format(-mltest.loglike(w1)))\n",
    "print(\"CC: {:.3f} / {:.3f} ({:.3f})\".format(psth_corr, eo, psth_corr/eo))\n",
    "print(\"spike count: data = {}, pred = {}\".format(test_data[\"spike_v\"].sum() / config[\"data\"][\"n_trials\"], pred.sum() / n_ppost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(saveplace + cell + \"_\" + tag,chain=sampler.flatchain,lnprob=sampler.flatlnprobability,map=w1)"
   ]
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
