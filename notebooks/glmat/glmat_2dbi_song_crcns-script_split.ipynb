{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLMAT: 2D kernel, song stimuli, ML+MC estimation\n",
    "\n",
    "This notebook demonstrates the full assimilation technique using song stimuli. The song waveform is processed to a 2D spectrogram, then convolved with a 2D STRF to produce the \"voltage\" of the GLMAT model. The adaptation \"current\" is calculated by convolving the spike trains with two exponential kernels. The goal of the assimilation is to estimate the parameters of the RF and the adaptation kernels. The parameter count of the RF is minimized by using a low-rank approximation (i.e., an outer product of two vectors) and by projecting time into a basis set of raised cosine filters that are spaced exponentially.\n",
    "\n",
    "The approach is to use elastic-net penalized maximum-likelihood estimation to get a first guess at the parameters. The regularization parameters and rank are selected using cross-validation. Then MCMC is used to sample the posterior distribution of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import sys\n",
    "import imp\n",
    "import numpy as np\n",
    "\n",
    "import yaml\n",
    "import pickle\n",
    "\n",
    "import mat_neuron._model as mat\n",
    "from dstrf import strf, mle, io, performance, spikes\n",
    "import progressbar\n",
    "\n",
    "# plotting packages\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt # plotting functions\n",
    "import seaborn as sns           # data visualization package\n",
    "sns.set_style(\"whitegrid\")\n",
    "mpl.rcParams['image.origin'] = 'lower'\n",
    "mpl.rcParams['image.aspect'] = 'auto'\n",
    "mpl.rcParams['image.cmap'] = 'jet'\n",
    "\n",
    "\n",
    "tag = \"test\"\n",
    "saveplace = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../scripts/quick.yml\",\"r\") as yf:\n",
    "    config = yaml.load(yf)\n",
    "    \n",
    "# set variables based on `config`\n",
    "ntaus = len(config[\"mat\"][\"taus\"])\n",
    "mat_fixed = np.asanyarray(config[\"mat\"][\"taus\"] + [config[\"mat\"][\"refract\"]],dtype='d')\n",
    "upsample = int(config[\"strf\"][\"stim_dt\"] / config[\"mat\"][\"model_dt\"])\n",
    "kcosbas = strf.cosbasis(config[\"strf\"][\"ntau\"], config[\"strf\"][\"ntbas\"])\n",
    "ntbas = kcosbas.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load some data from a real neural recording from the CRCNS dataset. To simplify the model, we concatenate the stimuli, setting padding between the stimuli sufficient to capture any offset responses. Note that the spike responses are convolved with the adaptation kernels before merging stimuli so that we don't inadvertently carry over spike history from trials that are not truly contiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('split-test.dat', 'wb') as outfile:\n",
    "#     pickle.dump(dict(w0=w0,\n",
    "#                  assim_data=assim_data,\n",
    "#                  test_data=test_data,\n",
    "#                  rf_lambda=rf_lambda,\n",
    "#                  rf_alpha=rf_alpha,\n",
    "#                  krank=krank,\n",
    "#                  mlest=mlest,\n",
    "#                  mltest=mltest),\n",
    "#                  outfile,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('split-test.dat', 'rb') as interfile:\n",
    "    ml_data = pickle.load(interfile)\n",
    "w0 = ml_data[\"w0\"]\n",
    "assim_data = ml_data[\"assim_data\"]\n",
    "test_data = ml_data[\"test_data\"]\n",
    "rf_lambda = ml_data[\"rf_lambda\"]\n",
    "rf_alpha = ml_data[\"rf_alpha\"]\n",
    "krank = ml_data[\"krank\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlest = mle.matfact(assim_data[\"stim\"], kcosbas, krank, assim_data[\"spike_v\"], assim_data[\"spike_h\"],\n",
    "                        assim_data[\"stim_dt\"], assim_data[\"spike_dt\"], \n",
    "                        nlin=config[\"mat\"][\"nlin\"])\n",
    "\n",
    "mltest = mle.matfact(test_data[\"stim\"], kcosbas, krank, test_data[\"spike_v\"], test_data[\"spike_h\"],\n",
    "                     test_data[\"stim_dt\"], test_data[\"spike_dt\"], nlin=config[\"mat\"][\"nlin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate parameters using emcee\n",
    "from neurofit import priors, costs, utils, startpos\n",
    "\n",
    "# the MAT parameters are just bounded between reasonable limits. These may need to be expanded when using real data\"][\n",
    "mat_prior = priors.joint_independent(\n",
    "                [ priors.uniform(config[\"mat\"][\"bounds\"][0][0], config[\"mat\"][\"bounds\"][0][1]),\n",
    "                  priors.uniform(config[\"mat\"][\"bounds\"][1][0], config[\"mat\"][\"bounds\"][1][1]),\n",
    "                  priors.uniform(config[\"mat\"][\"bounds\"][2][0], config[\"mat\"][\"bounds\"][2][1]),\n",
    "                ])\n",
    "\n",
    "def lnpost(theta):\n",
    "    \"\"\"Posterior probability for dynamical parameters\"\"\"\n",
    "    mparams = theta[:3]\n",
    "    rfparams = theta[3:]\n",
    "    ll = mat_prior(mparams)\n",
    "    if not np.isfinite(ll):\n",
    "        return -np.inf\n",
    "    w = np.r_[mparams, rfparams]\n",
    "    ll -= mlest.loglike(w, rf_lambda, rf_alpha)\n",
    "    return -np.inf if not np.isfinite(ll) else ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "if sys.platform == 'darwin':\n",
    "    config[\"emcee\"][\"nthreads\"] = 1\n",
    "\n",
    "# initialize walkers\n",
    "pos = p0 = startpos.normal_independent(config[\"emcee\"][\"nwalkers\"], w0, np.abs(w0) * 2)\n",
    "# initialize the sampler\n",
    "sampler = emcee.EnsembleSampler(config[\"emcee\"][\"nwalkers\"], w0.size, lnpost, \n",
    "                                threads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the sampler\n",
    "tracker = utils.convergence_tracker(config[\"emcee\"][\"nsteps\"], 10)\n",
    "for pos, prob, like in tracker(sampler.sample(pos, iterations=1, storechain=True)): \n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lnpost of p median: -21898.242634011054\n",
      "average acceptance fraction: 0.37\n",
      "[ 5.04353157 -1.3322361   0.0157891 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"lnpost of p median: {}\".format(np.median(prob)))\n",
    "print(\"average acceptance fraction: {}\".format(sampler.acceptance_fraction.mean()))\n",
    "try:\n",
    "    print(\"autocorrelation time: {}\".format(sampler.acor))\n",
    "except:\n",
    "    pass    \n",
    "w1 = np.median(pos, 0)\n",
    "rfparams = w1[3:]\n",
    "rf_map = strf.from_basis(mlest.strf(w0), kcosbas)\n",
    "print(w1[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emcee:\n",
      "loglike: -3953.008\n",
      "CC: -0.031 / 0.340 (-0.091)\n",
      "spike count: data = 69.3, pred = 679.6\n"
     ]
    }
   ],
   "source": [
    "n_ppost = 10\n",
    "mat.random_seed(1)\n",
    "t_stim = np.linspace(0, test_data[\"duration\"], test_data[\"stim\"].shape[1])\n",
    "    \n",
    "samples = np.random.permutation(config[\"emcee\"][\"nwalkers\"])[:n_ppost]\n",
    "pred = np.zeros((test_data[\"spike_v\"].shape[0], n_ppost), dtype=test_data[\"spike_v\"].dtype)\n",
    "for i, idx in enumerate(samples):\n",
    "    mparams = pos[idx]\n",
    "    V_mc = mltest.V(mparams)\n",
    "    pred[:, i] = mltest.predict(mparams, mat_fixed, V_mc)\n",
    "    spk_t = pred[:, i].nonzero()[0]\n",
    "\n",
    "pred_psth = spikes.psth(pred, upsample, 1)\n",
    "test_psth = spikes.psth(test_data[\"spike_v\"], upsample, 1)\n",
    "\n",
    "psth_corr = np.corrcoef(test_psth, pred_psth)[0, 1]\n",
    "eo = performance.corrcoef(test_data[\"spike_v\"][::2], test_data[\"spike_v\"][1::2], upsample, 1)\n",
    "\n",
    "print(\"emcee:\")\n",
    "print(\"loglike: {:.3f}\".format(-mltest.loglike(w1)))\n",
    "print(\"CC: {:.3f} / {:.3f} ({:.3f})\".format(psth_corr, eo, psth_corr/eo))\n",
    "print(\"spike count: data = {}, pred = {}\".format(test_data[\"spike_v\"].sum() / config[\"data\"][\"n_trials\"], pred.sum() / n_ppost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
