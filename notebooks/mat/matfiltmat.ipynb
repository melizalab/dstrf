{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import mat_neuron.core as matmodel\n",
    "\n",
    "# plotting packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt # plotting functions\n",
    "import seaborn as sns           # data visualization package\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters: (\u03b11, \u03b12, \u03b2, \u03c9, R, \u03c4m, \u03c41, \u03c42, \u03c4V, tref)\n",
    "matparams = np.asarray([100, 2, -0.3, 7, 10, 10, 10, 200, 5, 2], dtype='d')\n",
    "# these parameters should be slightly better but produce same output\n",
    "# matparams = np.asarray([9.27063294,   1.77843253,  -0.29152392,   6.26916095,  13.37407994,\n",
    "#                        10, 10, 200, 5, 2], dtype='d')\n",
    "matstate = np.zeros(6, dtype='d')\n",
    "model_dt = 0.5\n",
    "\n",
    "# data parameters\n",
    "duration = 20000\n",
    "n_samples = int(duration / model_dt)\n",
    "n_assim = 3\n",
    "n_test = 0\n",
    "\n",
    "# assimilation parameters\n",
    "nthreads = 4\n",
    "nwalkers = 2000\n",
    "nsteps = 500\n",
    "matparams_i = [0,1,2,3]\n",
    "matparams_n = len(matparams_i)\n",
    "matparams_f = matparams[matparams_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution - simple alpha kernel\n",
    "from scipy.signal import resample\n",
    "stim_dt = 10.0\n",
    "upsample = int(stim_dt / model_dt)\n",
    "\n",
    "# alpha filter\n",
    "tau_h = 50\n",
    "tt = np.arange(0, 600, stim_dt)\n",
    "ka = np.flipud(tt / tau_h * np.exp(-tt / tau_h))\n",
    "\n",
    "# difference of gammas:\n",
    "from scipy.special import gamma\n",
    "tau_h1 = 600. / 32\n",
    "tau_h2 = 600. / 16\n",
    "kg1 = 1/(gamma(6)*tau_h1)*(tt/tau_h1)**5 * np.exp(-tt/tau_h1)\n",
    "kg2 = 1/(gamma(6)*tau_h2)*(tt/tau_h2)**5 * np.exp(-tt/tau_h2)\n",
    "kg = np.flipud(kg1 - kg2 / 1.5)\n",
    "kg /= np.linalg.norm(kg)\n",
    "\n",
    "plt.plot(kg)\n",
    "plt.plot(ka)\n",
    "\n",
    "# select a filter\n",
    "k1 = ka\n",
    "def filter_stimulus(S, k1):\n",
    "    return np.convolve(S, k1, mode=\"same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data to fit\n",
    "np.random.seed(1)\n",
    "matmodel.random_seed(1)\n",
    "data = []\n",
    "stim = np.random.randn(int(n_samples / (stim_dt / model_dt)))\n",
    "stim[:100] = 0\n",
    "I = filter_stimulus(stim, k1)\n",
    "#I = np.random.randn(n_samples)\n",
    "for i in range(n_assim + n_test):\n",
    "    states, spikes = matmodel.predict(matstate, matparams, I, model_dt, upsample=upsample, stochastic=True)\n",
    "    z = np.nonzero(spikes)[0]\n",
    "    d = {\"I_stim\": I,\n",
    "         \"duration\": duration,\n",
    "         \"states\": states,\n",
    "         \"spike_t\": z, \n",
    "         \"spike_v\": spikes,\n",
    "         #\"lci\": matmodel.log_intensity(matstate, matparams, I, z, model_dt)\n",
    "         }\n",
    "    data.append(d)\n",
    "\n",
    "# split into assimilation and test sets\n",
    "assim_data = data[:n_assim]\n",
    "test_data = data[n_assim:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(data):\n",
    "    plt.vlines(d[\"spike_t\"], i, i + 0.5)\n",
    "plt.xlim([0, 5000])\n",
    "len(data[0][\"spike_t\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate parameters using emcee\n",
    "from neurofit import priors, costs, utils, startpos\n",
    "import emcee\n",
    "\n",
    "mat_prior = priors.joint_independent(\n",
    "                [ priors.uniform(-50,  200),\n",
    "                  priors.uniform(-5,   10),\n",
    "                  priors.uniform(-5,   5),\n",
    "                  priors.uniform( 0,  20),\n",
    "                  #priors.uniform( 5,  20),\n",
    "                ])\n",
    "startparams = np.asarray([[-50, 200],\n",
    "                          [-5, 10],\n",
    "                          [-5, 5],\n",
    "                          [0, 20],\n",
    "                          #[5, 20],\n",
    "                         ], dtype='d')\n",
    "\n",
    "# lasso prior on RF parameters\n",
    "rf_lambda = 1.0\n",
    "\n",
    "# this is the local copy of the parameters that we'll update in each step\n",
    "mparams = matparams.copy()\n",
    "rfparams = k1.copy()\n",
    "\n",
    "def lnpost_dyn(theta):\n",
    "    \"\"\"Posterior probability for dynamical parameters\"\"\"\n",
    "    from mat_neuron._model import lci_poisson\n",
    "    mparams[matparams_i] = theta[:matparams_n]\n",
    "    rfparams[:] = theta[matparams_n:]\n",
    "    rf_prior = -np.sum(np.abs(rfparams)) * rf_lambda\n",
    "    ll = mat_prior(theta[:matparams_n]) + rf_prior\n",
    "    if not np.isfinite(ll):\n",
    "        return -np.inf\n",
    "    lp = 0\n",
    "    I = filter_stimulus(stim, rfparams)\n",
    "    for d in assim_data:\n",
    "        lp += lci_poisson(matstate, mparams, I, d[\"spike_v\"], model_dt, upsample)\n",
    "    return ll + lp        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theoretically this is as good as it can get\n",
    "theta_true = np.concatenate([matparams_f, k1])\n",
    "print(\"lnpost of p_true: {}\".format(lnpost_dyn(theta_true)))\n",
    "# and this is our initial state\n",
    "p0 = np.concatenate([startpos.uniform_independent(nwalkers, startparams[:,0], startparams[:,1]),\n",
    "                     startpos.normal_independent(nwalkers, k1, [0.1] * k1.size)],\n",
    "                   axis=1)\n",
    "theta_0 = np.median(p0, 0)\n",
    "print(\"lnpost of p0 median: {}\".format(lnpost_dyn(theta_0)))\n",
    "%timeit lnpost_dyn(theta_true)\n",
    "%time for theta_0 in p0: lnpost_dyn(theta_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = emcee.EnsembleSampler(nwalkers, theta_true.size, lnpost_dyn, threads=nthreads)\n",
    "tracker = utils.convergence_tracker(nsteps, 25)\n",
    "\n",
    "for pos, prob, _ in tracker(sampler.sample(p0, iterations=nsteps, storechain=False)): \n",
    "    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"lnpost of p median: {}\".format(np.median(prob)))\n",
    "print(\"average acceptance fraction: {}\".format(sampler.acceptance_fraction.mean()))\n",
    "theta = np.median(pos, 0)\n",
    "mparams[matparams_i] = theta[:matparams_n]\n",
    "rfparams[:] = theta[matparams_n:]\n",
    "plt.plot(k1)\n",
    "plt.plot(rfparams)\n",
    "print(matparams_f)\n",
    "print(theta[:matparams_n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corner import corner\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "mpos = pos[:,:matparams_n]\n",
    "matlabs = ['a1','a2','b','w', 'R']\n",
    "c = corner(mpos,\n",
    "           range=[sp for sp in startparams],\n",
    "       bins=50, smooth=2,smooth1d=0,\n",
    "       labels=matlabs,\n",
    "       truths=matparams_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpos = pos[:,matparams_n:]\n",
    "c = corner(rpos[:,-6:],\n",
    "           range=[[-1, 1] for i in range(6)],\n",
    "           bins=50, smooth=2,smooth1d=0,\n",
    "           truths=k1[-6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how well predictions line up\n",
    "d = assim_data[0]\n",
    "\n",
    "Y, S = matmodel.predict(matstate, mparams, I, model_dt, upsample=upsample, stochastic=True)\n",
    "plt.plot(d[\"states\"][:4000,1])\n",
    "plt.plot(Y[:4000,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d[\"states\"][:4000,0])\n",
    "plt.plot(d[\"states\"][:4000,0] - d[\"states\"][:4000,4])\n",
    "plt.plot(Y[:4000,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(data):\n",
    "    plt.vlines(d[\"spike_t\"], i, i + 0.5, 'r')\n",
    "\n",
    "for i in range(len(data), len(data) + 10):\n",
    "    Y, S = matmodel.predict(matstate, mparams, I, model_dt, upsample=upsample, stochastic=True)\n",
    "    spk_t = S.nonzero()[0]\n",
    "    plt.vlines(spk_t, i, i + 0.5)\n",
    "\n",
    "plt.xlim(0, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(d[\"lci\"] - lci_guess).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "git": {
   "suppress_outputs": true
  },
  "kernelspec": {
   "display_name": "dstrf",
   "language": "python",
   "name": "dstrf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}